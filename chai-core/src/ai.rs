use crate::http::{get_client, strip_markdown_json};
use crate::models::{AIResponse, LLMResponse, SearchResult, Tea, TeaCard};
use crate::qdrant::{self, SearchFilters};
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tracing::{info, warn};

/// Maximum allowed query length to prevent abuse
const MAX_QUERY_LENGTH: usize = 1000;

/// LLM model used for tea recommendations
const MODEL: &str = "google/gemini-2.5-flash-lite";

/// Default number of teas to recommend if user doesn't specify
const DEFAULT_RESULT_COUNT: usize = 3;

/// Extra candidates to fetch from Qdrant (buffer for RAG errors)
const SEARCH_BUFFER: usize = 4;

/// Maximum number of teas user can request
const MAX_RESULT_COUNT: usize = 10;

/// Maximum tokens for query analysis (small response)
const MAX_ANALYSIS_TOKENS: u32 = 300;

/// Maximum tokens for final recommendation response
const MAX_RESPONSE_TOKENS: u32 = 1200;

/// Temperature for LLM sampling
const LLM_TEMPERATURE: f32 = 0.7;

#[derive(Serialize)]
struct OpenRouterRequest {
    model: String,
    messages: Vec<Message>,
    response_format: ResponseFormat,
    temperature: f32,
    max_tokens: u32,
}

#[derive(Serialize)]
struct Message {
    role: String,
    content: String,
}

#[derive(Serialize)]
struct ResponseFormat {
    #[serde(rename = "type")]
    format_type: String,
}

#[derive(Deserialize)]
struct OpenRouterResponse {
    choices: Vec<Choice>,
}

#[derive(Deserialize)]
struct Choice {
    message: ResponseMessage,
}

#[derive(Deserialize)]
struct ResponseMessage {
    content: String,
}

/// Query analysis result from first LLM call
#[derive(Debug, Deserialize)]
struct QueryAnalysis {
    /// Optimized search query for vector search
    search_query: String,
    /// Number of teas user wants (default: 3)
    #[serde(default)]
    result_count: Option<usize>,
    /// Exclude sample products (пробники)
    #[serde(default)]
    exclude_samples: bool,
    /// Exclude sets/bundles (наборы)
    #[serde(default)]
    exclude_sets: bool,
    /// Only show products in stock
    #[serde(default)]
    only_in_stock: bool,
}

/// Helper to call OpenRouter API
async fn call_llm(api_key: &str, prompt: &str, max_tokens: u32) -> Result<String> {
    use std::time::Instant;

    let client = get_client();
    let start = Instant::now();

    let request = OpenRouterRequest {
        model: MODEL.to_string(),
        messages: vec![Message {
            role: "user".to_string(),
            content: prompt.to_string(),
        }],
        response_format: ResponseFormat {
            format_type: "json_object".to_string(),
        },
        temperature: LLM_TEMPERATURE,
        max_tokens,
    };

    let response = client
        .post("https://openrouter.ai/api/v1/chat/completions")
        .header("Authorization", format!("Bearer {}", api_key))
        .header("Content-Type", "application/json")
        .json(&request)
        .send()
        .await?;

    let duration_ms = start.elapsed().as_millis();

    if !response.status().is_success() {
        let status = response.status();
        let text = response.text().await.unwrap_or_default();
        warn!(
            status = %status,
            duration_ms = %duration_ms,
            "LLM API error"
        );
        anyhow::bail!("OpenRouter API error {}: {}", status, text);
    }

    let mut result: OpenRouterResponse = response.json().await?;
    let content = result
        .choices
        .pop()
        .ok_or_else(|| anyhow::anyhow!("No response from AI"))?
        .message
        .content;

    info!(
        model = %MODEL,
        max_tokens = %max_tokens,
        duration_ms = %duration_ms,
        "LLM call completed"
    );

    Ok(content)
}

/// Stage 1: Analyze user query and extract search parameters
async fn analyze_query(user_query: &str, api_key: &str) -> Result<QueryAnalysis> {
    let prompt = format!(
        r#"Проанализируй запрос пользователя и извлеки параметры для поиска чая.

Запрос: "{}"

Верни JSON:
{{
  "search_query": "оптимизированный поисковый запрос для векторного поиска",
  "result_count": 3,
  "exclude_samples": false,
  "exclude_sets": false,
  "only_in_stock": false
}}

Правила:
- search_query: перефразируй для векторного поиска, оставь суть (вкусы, ингредиенты, эффекты, настроение)
- result_count: сколько чаёв хочет пользователь (по умолчанию 3, максимум 10). Ключевые слова: "один чай"=1, "пару"=2, "несколько"=3, "много"=5
- exclude_samples: true если НЕ хочет пробники
- exclude_sets: true если НЕ хочет наборы ("не набор", "без набора", "отдельный чай")
- only_in_stock: true если хочет только то, что есть в наличии

Только JSON."#,
        user_query
    );

    info!("Stage 1: Analyzing query");
    let content = call_llm(api_key, &prompt, MAX_ANALYSIS_TOKENS).await?;
    let cleaned = strip_markdown_json(&content);

    serde_json::from_str(cleaned)
        .with_context(|| format!("Failed to parse query analysis: {}", cleaned))
}

/// Stage 2: Build recommendation prompt with search results
fn build_recommendation_prompt(
    user_query: &str,
    search_results: &[SearchResult],
    result_count: usize,
) -> String {
    let teas_list: Vec<String> = search_results
        .iter()
        .map(|r| {
            let tea_name = r.tea.name.as_deref().unwrap_or("Без названия");

            let comp_str = if r.tea.composition.is_empty() {
                "Не указан".to_string()
            } else {
                r.tea.composition.join(", ")
            };

            // Краткое описание (первые 150 символов)
            let short_desc = r
                .tea
                .description
                .as_ref()
                .map(|d| {
                    if d.chars().count() > 150 {
                        format!("{}...", d.chars().take(150).collect::<String>())
                    } else {
                        d.clone()
                    }
                })
                .unwrap_or_else(|| "Нет описания".to_string());

            let series_str = r.tea.series.as_deref().unwrap_or("-");
            let tags_str = if r.tea.search_tags.is_empty() {
                "-".to_string()
            } else {
                r.tea.search_tags.join(", ")
            };
            let price_str = r.tea.price.as_deref().unwrap_or("-");
            let stock_str = if r.tea.in_stock { "В наличии" } else { "Нет в наличии" };

            format!(
                "ID: {}\nНазвание: {}\nСерия: {}\nЦена: {}\nНаличие: {}\nСостав: {}\nТеги: {}\nОписание: {}",
                r.tea.id, tea_name, series_str, price_str, stock_str, comp_str, tags_str, short_desc
            )
        })
        .collect();

    let teas_text = teas_list.join("\n\n");

    format!(
        r#"Ты — уютный чайный советник. Выбери ровно {} лучших чаёв из списка для пользователя.

Запрос пользователя: "{}"

Доступные чаи:

{}

Верни JSON:
{{
  "answer": "Поэтичный ответ (2-4 предложения). Пиши тепло и образно. Используй 2-4 эмодзи.",
  "tea_ids": ["id1", "id2", ...],
  "tags": {{
    "id1": ["тег1", "тег2"],
    "id2": ["тег1", "тег2"]
  }},
  "descriptions": {{
    "id1": "Краткое описание (1-2 предложения)",
    "id2": "Краткое описание (1-2 предложения)"
  }}
}}

Правила:
- tea_ids: выбери ровно {} самых подходящих чаёв из списка
- tags: 2-4 коротких тега (ингредиенты, вкус, эффект)
- descriptions: 1-2 предложения о вкусе и настроении чая
- answer: тёплый, поэтичный тон, упомяни почему эти чаи подходят

Только JSON."#,
        result_count, user_query, teas_text, result_count
    )
}

/// Главная функция: получить рекомендации чаёв от AI (двухэтапный подход)
pub async fn chat_completion(
    user_query: String,
    api_key: String,
    config: &crate::Config,
) -> Result<AIResponse> {
    use std::time::Instant;
    let total_start = Instant::now();

    // Input validation
    let query = user_query.trim();
    if query.is_empty() {
        anyhow::bail!("Query cannot be empty");
    }
    if query.len() > MAX_QUERY_LENGTH {
        anyhow::bail!(
            "Query too long: {} characters (max {})",
            query.len(),
            MAX_QUERY_LENGTH
        );
    }

    // Stage 1: Analyze query and extract search parameters
    let analysis = analyze_query(query, &api_key).await?;

    // Determine result count with bounds
    let result_count = analysis
        .result_count
        .unwrap_or(DEFAULT_RESULT_COUNT)
        .clamp(1, MAX_RESULT_COUNT);

    // Search for N + buffer candidates
    let search_count = result_count + SEARCH_BUFFER;

    info!(
        "Query analysis: search='{}', count={}, exclude_samples={}, exclude_sets={}, only_in_stock={}",
        analysis.search_query,
        result_count,
        analysis.exclude_samples,
        analysis.exclude_sets,
        analysis.only_in_stock
    );

    // Stage 2: Search with filters
    let filters = SearchFilters {
        exclude_samples: analysis.exclude_samples,
        exclude_sets: analysis.exclude_sets,
        only_in_stock: analysis.only_in_stock,
    };

    info!(
        "Stage 2: Searching for {} candidates (user wants {})",
        search_count, result_count
    );
    let search_results =
        qdrant::search_teas_filtered(&analysis.search_query, search_count, &filters, config)
            .await?;

    if search_results.is_empty() {
        anyhow::bail!("No teas found matching your query");
    }

    info!("Found {} candidates", search_results.len());

    // Build lookup map
    let tea_map: HashMap<&str, (&Tea, f32)> = search_results
        .iter()
        .map(|r| (r.tea.id.as_str(), (&r.tea, r.score)))
        .collect();

    // Stage 3: Get recommendations from LLM
    let prompt = build_recommendation_prompt(query, &search_results, result_count);
    info!("Stage 3: Getting {} recommendations from LLM", result_count);
    let content = call_llm(&api_key, &prompt, MAX_RESPONSE_TOKENS).await?;

    // Parse LLM response
    let cleaned_content = strip_markdown_json(&content);
    let llm_response: LLMResponse = serde_json::from_str(cleaned_content)
        .with_context(|| format!("Failed to parse LLM response as JSON: {}", cleaned_content))?;

    // Build tea cards from LLM selection
    let mut tea_cards = Vec::new();

    for tea_id in &llm_response.tea_ids {
        match tea_map.get(tea_id.as_str()) {
            Some((tea, score)) => {
                let tags = llm_response.tags.get(tea_id).cloned().unwrap_or_default();
                let short_description = llm_response
                    .descriptions
                    .get(tea_id)
                    .cloned()
                    .unwrap_or_default();

                let card = TeaCard {
                    url: tea.url.clone(),
                    title: tea.name.clone().unwrap_or_default(),
                    tags,
                    match_score: *score,
                    short_description,
                    price: tea.price.clone(),
                    image_url: tea.images.first().cloned(),
                    in_stock: tea.in_stock,
                    composition: tea.composition.clone(),
                    sample_url: tea.sample_url.clone(),
                    sample_in_stock: false,
                    description: tea.description.clone(),
                    series: tea.series.clone(),
                    full_composition: tea.full_composition.clone(),
                    price_variants: tea.price_variants.clone(),
                };

                tea_cards.push(card);
            }
            None => {
                warn!("LLM returned unknown tea_id: {}", tea_id);
            }
        }
    }

    // Fetch sample stock status in parallel with timeout
    let sample_futures: Vec<_> = tea_cards
        .iter()
        .enumerate()
        .filter_map(|(idx, card)| card.sample_url.as_ref().map(|url| (idx, url.clone())))
        .map(|(idx, url)| async move {
            let result = qdrant::get_tea_by_url(&url, config).await;
            (idx, result)
        })
        .collect();

    const SAMPLE_LOOKUP_TIMEOUT_SECS: u64 = 10;
    match tokio::time::timeout(
        std::time::Duration::from_secs(SAMPLE_LOOKUP_TIMEOUT_SECS),
        futures::future::join_all(sample_futures),
    )
    .await
    {
        Ok(sample_results) => {
            for (idx, result) in sample_results {
                if let Ok(Some(sample_tea)) = result {
                    tea_cards[idx].sample_in_stock = sample_tea.in_stock;
                }
            }
        }
        Err(_) => {
            warn!(
                "Sample stock lookup timed out after {}s",
                SAMPLE_LOOKUP_TIMEOUT_SECS
            );
        }
    }

    let total_duration_ms = total_start.elapsed().as_millis();
    info!(
        query = %query,
        results = tea_cards.len(),
        total_duration_ms = %total_duration_ms,
        "AI pipeline completed"
    );

    Ok(AIResponse {
        answer: llm_response.answer,
        tea_cards,
    })
}
